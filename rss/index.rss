<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Du Traitement des Données</title><description>Data and Julia.</description><link>http://localhost:2368/</link><generator>Ghost 0.11</generator><lastBuildDate>Fri, 28 Oct 2016 06:20:38 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>DataStreams.jl v0.1</title><description>&lt;p&gt;It's been almost exactly a year since my original &lt;a href="http://julialang.org/blog/2015/10/datastreams"&gt;blog post&lt;/a&gt; about DataStreams.jl, so I figure it's time for an update!&lt;/p&gt;

&lt;p&gt;There's actually quite a bit to announce and I'm really excited with how things have developed over the last year.&lt;/p&gt;

&lt;h5 id="whatisdatastreamsjlagain"&gt;What is DataStreams.jl again?&lt;/h5&gt;

&lt;p&gt;&lt;a href="https://github.com/JuliaData/DataStreams.jl"&gt;DataStreams.jl&lt;/a&gt; is&lt;/p&gt;</description><link>http://localhost:2368/datastreams-jl-v0-1/</link><guid isPermaLink="false">6e5dfa21-1bb4-449f-bd7c-e8f1f4615572</guid><dc:creator>Jacob Quinn</dc:creator><pubDate>Fri, 28 Oct 2016 06:20:28 GMT</pubDate><content:encoded>&lt;p&gt;It's been almost exactly a year since my original &lt;a href="http://julialang.org/blog/2015/10/datastreams"&gt;blog post&lt;/a&gt; about DataStreams.jl, so I figure it's time for an update!&lt;/p&gt;

&lt;p&gt;There's actually quite a bit to announce and I'm really excited with how things have developed over the last year.&lt;/p&gt;

&lt;h5 id="whatisdatastreamsjlagain"&gt;What is DataStreams.jl again?&lt;/h5&gt;

&lt;p&gt;&lt;a href="https://github.com/JuliaData/DataStreams.jl"&gt;DataStreams.jl&lt;/a&gt; is about designing interfaces for easy and efficient transfer of "table-like" data (i.e. data that can, at least in some sense, be described by rows and columns) between sources and sinks. The key is to provide an interface (i.e. set of required methods to implement) such that as long as a source correctly implements, it can now "stream" data to any existing, valid sink. Similarly, as long as a sink implements the required "sink interface" methods, it should then be able to "receive" data from any source. I gave an intro to the framework in my &lt;a href="https://www.youtube.com/watch?v=N39V6JMWazo"&gt;2016 JuliaCon talk&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Traditionally, sources and sinks have been developed in isolation, with a source or sink having to provide its own options for interacting with other sources/sinks, usually only a single option (e.g. reading a csv file read in as a dataframe, or loading a csv file into a database). This is pain-staking work, however, to support for each package independently. Imagine as a database middleware developer, do I really want to go through the effort of allowing users to load data into a DB via csv file, sqlite table, dataframe, etc.? A natural first step is to stop and say, "well, I can't possibly support all existing or future types of sources to load into a DB, but I could define a common interface, and as long as a source implements the interface, I can load it into the db". All the sudden, this dev relieves herself of having to be an expert in the most-efficient handling of a specific datasource type, opting instead to allow source developers to code to an interface.&lt;/p&gt;

&lt;p&gt;This line of reasoning has been taken even several steps further in the maturation of the DataStreams.jl framework. Independent interfaces exist for both &lt;code&gt;Data.Source&lt;/code&gt; types, as well as &lt;code&gt;Data.Sink&lt;/code&gt;s. DataStreams.jl itself provides a set of common &lt;code&gt;Data.stream!&lt;/code&gt; methods that handle the actual initialization, streaming, and closedown of the data ingestion process, but has and needs no outside knowledge of the specific Source/Sink implementations. For a real-life example, the &lt;a href="https://github.com/JuliaData/CSV.jl"&gt;CSV.jl&lt;/a&gt; package now provides a &lt;code&gt;CSV.Source&lt;/code&gt; type which implements the &lt;code&gt;Data.Source&lt;/code&gt; interface and concerns itself solely with producing table-like data as efficiently as possible. &lt;code&gt;CSV.Sink&lt;/code&gt;, on the other hand, implements the &lt;code&gt;Data.Sink&lt;/code&gt; interface and only has to worry about storing table-data to disk in the csv format. One of the most powerful results of this Source-Sink decoupling is the fact that adding a new Source or Sink into the ecosystem is &lt;code&gt;O(1)&lt;/code&gt;, i.e. I don't have to worry about whether my new source type will be compatible with outputting data to 10 different sink types. I define how my source provides data, ensure the &lt;code&gt;Data.Source&lt;/code&gt; interface is properly implemented, and boom, my new source can now interact with any existing and future sinks, automatically and efficiently.&lt;/p&gt;

&lt;p&gt;For a more technical and in-depth reading of the interfaces for &lt;code&gt;Data.Source&lt;/code&gt; or &lt;code&gt;Data.Sink&lt;/code&gt;, see the docs &lt;a href="http://juliadata.github.io/DataStreams.jl/stable/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id="newdatastreamsfeatures"&gt;New DataStreams Features&lt;/h5&gt;

&lt;p&gt;In addition to the power of Source-Sink decoupling, having the actual streaming framework defined centrally in DataStreams.jl allows for several other useful features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Appending&lt;/strong&gt;: oftentimes, a user needs to aggregate several sources into a single sink; as part of the &lt;code&gt;Data.Sink&lt;/code&gt; interface, sinks must account for the streaming scenario where new source data will be appended to existing sink data (with matching schema as an obvious requirement)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transforms&lt;/strong&gt;: often, users need to apply simple data cleansing or manipulating tasks to source data before it's in a useable form to analyze or output to a sink. With the data transfer defined centrally, it was extremely simple to allow the user to supply a set of transform functions that should be applied to the data "in-transit", i.e. as a single field value or column of values is provided by the &lt;code&gt;Data.Source&lt;/code&gt;, the transform function is applied before sending to the &lt;code&gt;Data.Sink&lt;/code&gt; for storage. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Package use consistency&lt;/strong&gt;: due to the consistency of implementing the &lt;code&gt;Data.Source&lt;/code&gt; and &lt;code&gt;Data.Sink&lt;/code&gt; interfaces separately, each package has a natural "high-level" convenience method fall out. Such methods for Sources are:
&lt;ul&gt;&lt;li&gt;&lt;code&gt;CSV.read(file, sink=DataFrame; append=false, transforms=Dict())&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SQL.query(db, query, sink=DataFrame; append=false, transforms=Dict())&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Feather.read(file, sink=DataFrame; append=false, transforms=Dict())&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ODBC.query(dsn, query, sink=DataFrame; append=false, transforms=Dict())&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The definition for these convenience methods are almost identical across packages, utilizing the same DataStreams.jl infrastructure, and all allowing the common DataStream features such as specifying any &lt;code&gt;sink&lt;/code&gt; argument they'd like (typically a &lt;code&gt;DataFrame&lt;/code&gt; by default), whether to &lt;code&gt;append&lt;/code&gt; to the sink or not, and any &lt;code&gt;transform&lt;/code&gt; functions that should be applied along the way. These convenience methods work by constructing &lt;code&gt;Data.Source&lt;/code&gt; (and possibly &lt;code&gt;Data.Sink&lt;/code&gt;) types "on-the-fly" before calling &lt;code&gt;Data.stream!(source, sink)&lt;/code&gt; and &lt;code&gt;Data.close!(sink)&lt;/code&gt; to perform the actual data transfer. &lt;/p&gt;

&lt;p&gt;Similar convenience methods for &lt;code&gt;Data.Sink&lt;/code&gt; are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CSV.write(file, source)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SQLite.load(db, tablename, source)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Feather.write(file, source)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ODBC.load(dsn, tablename)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with all supporting the same &lt;code&gt;append&lt;/code&gt; and &lt;code&gt;transforms&lt;/code&gt; keyword arguments as well.&lt;/p&gt;

&lt;h5 id="whatsnext"&gt;What's next:&lt;/h5&gt;

&lt;p&gt;Things on my mind and that will be taking up my mind include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;DataFrames modernizing: there's some major work already underway to improve the core DataFrame type for handling tabular data in Julia &lt;/li&gt;
&lt;li&gt;Multi-source/sink streaming: though the framework already supports &lt;code&gt;append&lt;/code&gt;ing when streaming, there's more in specific use-cases that could be supported by DataStreams centrally&lt;/li&gt;
&lt;li&gt;More formal and powerful transforms: I feel like we've only scratched the surface for transforms so far; I don't think it'll be too difficult to support thinks like: new-column transforms, multi-column transforms, and possibly aggregation transforms.&lt;/li&gt;
&lt;li&gt;More Sources &amp;amp; Sinks!: I hope to help enable as many as possible, such as SAS, RData, certain geo-data formats, etc. Stream all the things!!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, I definitely want to give shoutouts to those who have helped, with code, design, or otherwise: &lt;a href="https://github.com/davidagold"&gt;David Gold&lt;/a&gt;, &lt;a href="https://github.com/johnmyleswhite"&gt;John Myles White&lt;/a&gt;, &lt;a href="https://github.com/nalimilan"&gt;Milan Bouchet-Valat&lt;/a&gt;, &lt;a href="https://github.com/davidanthoff"&gt;David Anthoff&lt;/a&gt;, and &lt;a href="https://github.com/yeesian"&gt;Yeesian Ng&lt;/a&gt; for their direct collaboration and help, as well as many of the core contributors to &lt;a href="https://github.com/JuliaLang/julia/graphs/contributors"&gt;julia itself&lt;/a&gt;. And special shoutouts to Tom Breloff, Jiahao Chen, Cameron McBride, and Jeff Bezanson for specific discussions and suggestions at JuliaCon 2016 and since.&lt;/p&gt;

&lt;p&gt;I'm also including some quick "new release news" for individual packages below, since there have been quite a few new features added:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CSV.jl&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;a href="http://juliadata.github.io/CSV.jl/stable/"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Supports a wide variety of delimited file options such as &lt;code&gt;delim&lt;/code&gt;, &lt;code&gt;quotechar&lt;/code&gt;, &lt;code&gt;escapechar&lt;/code&gt;, custom &lt;code&gt;null&lt;/code&gt; strings; a &lt;code&gt;header&lt;/code&gt; can be provided manually or on a specified row or range of rows; &lt;code&gt;types&lt;/code&gt; can be provided manually, and results can be requested as &lt;code&gt;nullable&lt;/code&gt; or not (&lt;code&gt;nullable=true&lt;/code&gt; by default); and the # of &lt;code&gt;rows&lt;/code&gt; can be provided manually (if known) for efficiency.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CSV.parsefield(io::IO, ::Type{T})&lt;/code&gt; can be called directly on any &lt;code&gt;IO&lt;/code&gt; type to tap into the delimited-parsing functionality manually&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SQLite.jl&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;a href="http://juliadb.github.io/SQLite.jl/stable/"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Query results will now use the declared table column type by default, which can help resultset column typing in some cases&lt;/li&gt;
&lt;li&gt;Parameterized SQL statements are fully supported, with the ability to bind julia values to be sent to the DB&lt;/li&gt;
&lt;li&gt;Full serialization/deserialization of native and custom Julia types is supported; so &lt;code&gt;Complex{Int128}&lt;/code&gt; can be stored in its own SQLite table column and retrieved without any issue&lt;/li&gt;
&lt;li&gt;Pure Julia scalar and aggregation functions can be registered with an SQLite database and then called from within SQL statements: full docs &lt;a href="http://juliadb.github.io/SQLite.jl/stable/#User-Defined-Functions-1"&gt;here&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feather.jl&lt;/strong&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://juliastats.github.io/Feather.jl/stable/"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full support for feather release v0.3.0 to ensure compatibility&lt;/li&gt;
&lt;li&gt;Full support for returning "factor" or "category" type columns as native &lt;code&gt;CategoricalArray&lt;/code&gt; and &lt;code&gt;NullableCategoricalArray&lt;/code&gt; types in Julia, thanks to the new &lt;a href="https://github.com/JuliaData/CategoricalArrays.jl"&gt;CategoricalArrays.jl&lt;/a&gt; package&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nullable::Bool=true&lt;/code&gt; keyword argument; if &lt;code&gt;false&lt;/code&gt;, columns without null values will be returned as &lt;code&gt;Vector{T}&lt;/code&gt; instead of &lt;code&gt;NullableVector{T}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Feather.Sink&lt;/code&gt; now supports appending, so multiple DataFrames or CSV.Source or any &lt;code&gt;Data.Source&lt;/code&gt; can all be streamed to a single feather file&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ODBC.jl&lt;/strong&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://juliadb.github.io/ODBC.jl/stable/"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A new &lt;code&gt;ODBC.DSN&lt;/code&gt; type that represents a valid, open connection to a database; used in all subsequent api calls; it can be constructed using a previously configured system/user dsn w/ username and password, or as a full custom connection string&lt;/li&gt;
&lt;li&gt;Full support for the DataStreams.jl framework through the &lt;code&gt;ODBC.Source&lt;/code&gt; and &lt;code&gt;ODBC.Sink&lt;/code&gt; types, along with their high-level convenience methods &lt;code&gt;ODBC.query&lt;/code&gt; and &lt;code&gt;ODBC.load&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A new &lt;code&gt;ODBC.prepare(dsn, sql) =&amp;gt; ODBC.Statement&lt;/code&gt; method which can send an &lt;code&gt;sql&lt;/code&gt; statement to the database to be compiled and planned before executed 1 or more times. SQL statements can include parameters to be prepared that can have dynamic values bound before each execution.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title>JSoC 2015 Project: DataStreams.jl</title><description>&lt;p&gt;&lt;span class="timestamp"&gt;25 Oct 2015&lt;/span&gt;  |  &lt;span class="author"&gt;&lt;a href="https://github.com/quinnj"&gt;Jacob Quinn&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[[&lt;strong&gt;&lt;em&gt;Originally posted on the official &lt;a href="http://julialang.org/blog/2015/10/datastreams"&gt;JuliaLang blog&lt;/a&gt;]]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data processing got ya down? Good news! The &lt;a href="https://github.com/JuliaDB/DataStreams.jl"&gt;DataStreams.jl&lt;/a&gt; package, er, framework, has arrived!&lt;/p&gt;

&lt;p&gt;The DataStreams processing framework provides a consistent interface for working with data, from source to sink and eventually every step in-between. It’s&lt;/p&gt;</description><link>http://localhost:2368/jsoc-2015-project-datastreams-jl/</link><guid isPermaLink="false">0722c84a-f630-4789-bfe8-616378e19b2f</guid><dc:creator>Jacob Quinn</dc:creator><pubDate>Fri, 28 Oct 2016 04:34:45 GMT</pubDate><content:encoded>&lt;p&gt;&lt;span class="timestamp"&gt;25 Oct 2015&lt;/span&gt;  |  &lt;span class="author"&gt;&lt;a href="https://github.com/quinnj"&gt;Jacob Quinn&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[[&lt;strong&gt;&lt;em&gt;Originally posted on the official &lt;a href="http://julialang.org/blog/2015/10/datastreams"&gt;JuliaLang blog&lt;/a&gt;]]&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data processing got ya down? Good news! The &lt;a href="https://github.com/JuliaDB/DataStreams.jl"&gt;DataStreams.jl&lt;/a&gt; package, er, framework, has arrived!&lt;/p&gt;

&lt;p&gt;The DataStreams processing framework provides a consistent interface for working with data, from source to sink and eventually every step in-between. It’s really about putting forth an interface (specific types and methods) to go about ingesting and transferring data sources that hopefully makes for a consistent experience for users, no matter what kind of data they’re working with.&lt;/p&gt;

&lt;h6 id="howdoesitwork"&gt;How does it work?&lt;/h6&gt;

&lt;p&gt;DataStreams is all about creating “sources” (Julia types that represent true data sources; e.g. csv files, database backends, etc.), “sinks” or data destinations, and defining the appropriate &lt;code&gt;Data.stream!(source, sink)&lt;/code&gt; methods to actually transfer data from source to sink. Let’s look at a quick example.&lt;/p&gt;

&lt;p&gt;Say I have a table of data in a CSV file on my local machine and need to do a little cleaning and aggregation on the data before building a model with the &lt;a href="https://github.com/JuliaStats/GLM.jl"&gt;GLM.jl&lt;/a&gt; package. Let’s see some code in action:&lt;/p&gt;

&lt;pre&gt;&lt;code class="language-julia"&gt;using CSV, SQLite, DataStreams, DataFrames

# let's create a Julia type that understands our data file
csv_source = CSV.Source("datafile.csv")

# let's also create an SQLite destination for our data
# according to its structure
db = SQLite.DB() # create an in-memory SQLite database

# creates an SQLite table
sqlite_sink = SQLite.Sink(db, "mydata")

# parse the CSV data directly into our SQLite table
Data.stream!(csv_source, sqlite_sink)

# now I can do some data cleansing/aggregation
# ...various SQL statements on the "mydata" SQLite table...

# now I'm ready to get my data out and ready for model fitting
sqlite_source = SQLite.Source(sqlite_sink)

# stream our data into a Julia structure (Data.Table)
dt = Data.stream!(sqlite_source, Data.Table)

# convert to DataFrame (non-copying)
df = DataFrame(dt)

# do model-fitting
OLS = glm(Y~X,df,Normal(),IdentityLink())  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we see it’s quite simple to create a &lt;code&gt;Source&lt;/code&gt; type by wrapping a true datasource (our CSV file), a destination for that data (an SQLite table), and to transfer the data. We can then turn our &lt;code&gt;SQLite.Sink&lt;/code&gt; into an &lt;code&gt;SQLite.Source&lt;/code&gt; for getting the data back out again.&lt;/p&gt;

&lt;h5 id="sowhathaveyoureallybeenworkingon"&gt;So What Have You Really Been Working On?&lt;/h5&gt;

&lt;p&gt;Well, a lot actually. Even though the DataStreams framework is currently simple and minimalistic, it took a lot of back and forth on the design, including several discussions at this year’s JuliaCon at MIT. Even with a tidy little framework, however, the bulk of the work still lies in actually implementing the interface in various packages. The two that are ready for release today are &lt;a href="https://github.com/JuliaDB/CSV.jl"&gt;CSV.jl&lt;/a&gt; and &lt;a href="https://github.com/JuliaDB/SQLite.jl"&gt;SQLite.jl&lt;/a&gt;. They are currently available for julia 0.4+ only.&lt;/p&gt;

&lt;p&gt;Quick rundown of each package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CSV: provides types and methods for working with CSV and other delimited files. Aims to be (and currently is) the fastest and most flexible CSV reader in Julia.&lt;/li&gt;
&lt;li&gt;SQLite: an interface to the popular &lt;a href="http://sqlite.org/"&gt;SQLite&lt;/a&gt; local-machine database. Provides methods for creating/managing database files, along with executing SQL statements and viewing the results of such.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id="sowhatsnext"&gt;So What’s Next?&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/JuliaDB/ODBC.jl"&gt;ODBC.jl&lt;/a&gt;: the next package to get the DataStreams makeover is ODBC. I’ve already started work on this and hopefully should be ready soon.&lt;/li&gt;
&lt;li&gt;Other packages: I’m always on the hunt for new ways to spread the framework; if you’d be interested in implementing DataStreams for your own package or want to collaborate, just &lt;a href="https://github.com/quinnj"&gt;ping&lt;/a&gt; me and I’m happy to discuss!&lt;/li&gt;
&lt;li&gt;transforms: an important part of data processing tasks is not just connecting to and moving the data to somewhere else: often you need to clean/transform/aggregate the data in some way in-between. Right now, that’s up to users, but I have some ideas around creating DataStreams-friendly ways to easily incorporate transform steps as data is streamed from one place to another.&lt;/li&gt;
&lt;li&gt;DataStreams for chaining pipelines + transforms: I’m also excited about the idea of creating entire &lt;code&gt;DataStreams&lt;/code&gt;, which would define entire data processing tasks end-to-end. Setting up a pipeline that could consistently move and process data gets even more powerful as we start looking into automatic-parallelism and extensibility.&lt;/li&gt;
&lt;li&gt;DataStream scheduling/management: I’m also interested in developing capabilities around scheduling and managing DataStreams.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;The work on DataStreams.jl was carried out as part of the Julia Summer of Code program, made possible thanks to the generous support of the &lt;a href="https://moore.org"&gt;Gordon and Betty Moore Foundation&lt;/a&gt;, and MIT.&lt;/em&gt;&lt;/p&gt;</content:encoded></item><item><title>Deloitte AAM FARS Capstone Project</title><description>&lt;h4 id="background"&gt;Background&lt;/h4&gt;

&lt;p&gt;This &lt;a href="http://quinnj.github.io/heatmap.html"&gt;webapp&lt;/a&gt; was put together as apart of a capstone project for Deloitte AAM. The project involved utilizing analytics and machine learning to model fatal auto accident propensity. This webapp aims to provide insight on fatal accident densities by interfacing with public Google API's combined with the FARS dataset.&lt;/p&gt;</description><link>http://localhost:2368/untitled/</link><guid isPermaLink="false">4e2bce9d-d98c-4f86-bf4a-c340ab0447f7</guid><dc:creator>Jacob Quinn</dc:creator><pubDate>Fri, 28 Oct 2016 04:27:19 GMT</pubDate><content:encoded>&lt;h4 id="background"&gt;Background&lt;/h4&gt;

&lt;p&gt;This &lt;a href="http://quinnj.github.io/heatmap.html"&gt;webapp&lt;/a&gt; was put together as apart of a capstone project for Deloitte AAM. The project involved utilizing analytics and machine learning to model fatal auto accident propensity. This webapp aims to provide insight on fatal accident densities by interfacing with public Google API's combined with the FARS dataset.&lt;/p&gt;

&lt;h4 id="usage"&gt;Usage&lt;/h4&gt;

&lt;h6 id="navigationapp"&gt;Navigation App&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;Enter a starting and ending location&lt;/li&gt;
&lt;li&gt;Select time of day or weather filters&lt;/li&gt;
&lt;li&gt;Push the &lt;code&gt;Go!&lt;/code&gt; button&lt;/li&gt;
&lt;li&gt;Select time of day or weather filters to view differences&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id="10mileradiusapp"&gt;10-Mile Radius App&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;Enter a location&lt;/li&gt;
&lt;li&gt;Push the &lt;code&gt;Go!&lt;/code&gt; button&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="howitworks"&gt;How It Works&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Navigation&lt;/strong&gt; app queries the &lt;a href="https://developers.google.com/maps/documentation/directions/"&gt;Google Directions API&lt;/a&gt; to obtain the 1-3 recommended routes from point A to point B; the &lt;strong&gt;10-Mile Radius&lt;/strong&gt; app calculates the latitude and longitude bounds on the map after centering it on the entered location&lt;/li&gt;
&lt;li&gt;With the recommended routes, the FARS dataset of fatal auto accidents is processed to identify accidents that occurred along each route that satisfy any time of day or weather filters&lt;/li&gt;
&lt;li&gt;The applicable accidents are plotted along the navigation routes or in the 10-mile bounding box by utilizing the &lt;a href="https://developers.google.com/maps/documentation/javascript/heatmaplayer"&gt;Google Heatmap Layer API&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="definitions"&gt;Definitions&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Data source is the government-provided FARS data set of fatal auto accidents released each year. Data includes attributes of accidents and vehicles and people involved. (&lt;a href="http://www.nhtsa.gov/FARS"&gt;http://www.nhtsa.gov/FARS&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Any Time&lt;/code&gt;: Show fatal accidents that occurred at any time of day&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Morning&lt;/code&gt;: Show fatal accidents that occurred between 4 AM and 12 PM&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Afternoon&lt;/code&gt;: Show fatal accidents that occurred between 12 PM and 7 PM&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Night&lt;/code&gt;: Show fatal accidents that occurred between 7 PM and 4 AM&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Any Weather&lt;/code&gt;: Show fatal accidents that occurred under any weather condition&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Clear&lt;/code&gt;: Show fatal accidents that occurred under clear weather conditions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Adverse&lt;/code&gt;: Show fatal accidents that occurred under adverse weather conditions, including rain, snow, sleet, hail, severe wind, and fog.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title>Data Structures as Code: The Joys of Meta-Programming</title><description>&lt;p&gt;While working on a major &lt;a href="https://github.com/quinnj/Dates.jl"&gt;rewrite&lt;/a&gt; of the &lt;a href="https://github.com/quinnj/Datetime.jl"&gt;Datetime&lt;/a&gt; Julia library for Dates and DateTimes, increased performance was a key goal. And though the original implementation was already fast, Julia developers tend to be &lt;a href="http://julialang.org/blog/2012/02/why-we-created-julia/"&gt;greedy&lt;/a&gt;, so naturally I wanted more.&lt;/p&gt;

&lt;p&gt;One of the known bottlenecks at the time, was how&lt;/p&gt;</description><link>http://localhost:2368/welcome-to-ghost/</link><guid isPermaLink="false">5851b994-3c73-43a4-a100-9bcbef374604</guid><category>Getting Started</category><dc:creator>Jacob Quinn</dc:creator><pubDate>Fri, 28 Oct 2016 04:10:03 GMT</pubDate><content:encoded>&lt;p&gt;While working on a major &lt;a href="https://github.com/quinnj/Dates.jl"&gt;rewrite&lt;/a&gt; of the &lt;a href="https://github.com/quinnj/Datetime.jl"&gt;Datetime&lt;/a&gt; Julia library for Dates and DateTimes, increased performance was a key goal. And though the original implementation was already fast, Julia developers tend to be &lt;a href="http://julialang.org/blog/2012/02/why-we-created-julia/"&gt;greedy&lt;/a&gt;, so naturally I wanted more.&lt;/p&gt;

&lt;p&gt;One of the known bottlenecks at the time, was how &lt;a href="http://en.wikipedia.org/wiki/Leap_second"&gt;leap seconds&lt;/a&gt; were being dealt with. The &lt;code&gt;DateTime&lt;/code&gt; type accounted for leap seconds like any other second, and naturally this complicated the internals a bit. See, the internal type is just stored as a &lt;em&gt;machine instant&lt;/em&gt;, a monotonically increasing number from zero.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;julia&amp;gt; dt = datetime(2014,6,19)  
2014-06-19T00:00:00 UTC

julia&amp;gt; int(dt)  
63538819225000  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the tricky part, comes in the algorithm to calculate this machine instant given the parts of a date (year, month, day, etc.). In a leap-seconds-don't-exist kind of world, this is trivial:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;totalms = ms + 1000*(s + 60mi + 3600h + 86400*totaldays(y,m,d))  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The hard part, is when trying to convert the above into leap-seconds-&lt;em&gt;do&lt;/em&gt;-exist world.&lt;/p&gt;

&lt;p&gt;Without going into too much detail, the solution basically involves using two cached arrays, one in the leap second timeline specifying the instant a leap second occurred; while the other is on a non-leap second timeline specifying the instant right before a leap second would occur (yes, it totally makes me think of &lt;a href="http://en.wikipedia.org/wiki/Remedial_Chaos_Theory"&gt;alternate timelines&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The bottleneck problem boils down to the fact that every time you create or show a &lt;code&gt;DateTime&lt;/code&gt;, you have to do a lookup into these leap second arrays to figure out how many leap seconds need to be corrected in either setting the machine instant, or displaying the right date parts.&lt;/p&gt;

&lt;p&gt;The natural thing here is to employ binary search, which Base Julia provides an excellent implmentation of in the &lt;a href="http://docs.julialang.org/en/latest/stdlib/sort/#Base.searchsorted"&gt;searchsorted&lt;/a&gt; family of functions. Basically, we calculate our &lt;code&gt;totalms&lt;/code&gt; from above, then do binary search to figure out when the previous leap second instant occured and how many leap seconds it represents and need to be corrected for our calculated instant (in this case, we could use the &lt;code&gt;searchsortedlast&lt;/code&gt; function to get the last leap second instant before our calculated instant). Simple enough, right? End of story? Move on?&lt;/p&gt;

&lt;p&gt;But let's step back a minute and think about the nature of the problem. A major insight is the fact that our leap second arrays are static--nothing is being added or removed apart from library releases and when the &lt;a href="http://www.iers.org/"&gt;IERS&lt;/a&gt; decides things are getting too out of whack with earth's rotation. So how could we use this knowledge to improve performance? Well, if we take our &lt;code&gt;searchsortedlast&lt;/code&gt; binary search to the extreme, it would ideally look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;const LEAPSECONDS = [...]  
function setleaps(totalms)  
    if totalms &amp;lt; 62624707200000  # our middle value of LEAPSECONDS
        if totalms &amp;lt; 62388144000000 # cut 1st half in half again
            if totalms &amp;lt; 62230377600000 # last cut needed
                return 1  # only one leap second has occured
            else
                return 2  # 2 leap seconds
            end
        else
        ...
    else
    ...
end  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically, a hand-written &lt;code&gt;searchsortedlast&lt;/code&gt; binary search tree using our static array values that elimates all recursive/function call overhead and is close to the metal. But who wants to write all that out by hand? And when our leap second arrays &lt;em&gt;do&lt;/em&gt; get updated, we'll have to manually rebalance our tree, and hand-write it all over again.....yuck. But wait, I seem to recall a famous &lt;a href="https://github.com/JuliaLang/julia/pull/2987"&gt;PR&lt;/a&gt; by &lt;a href="http://math.mit.edu/~stevenj/"&gt;Steven Johnson&lt;/a&gt; where he used a macro for hand-unrolling array coefficients that other libraries wouldn't dream of trying to maintain. Perhaps we too can leverage Julia's meta-programming capabilities to do our dirty work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Recursively build binary search tree w/ known lookup values
# A is sorted array of lookup values
# R is return values for each index of A
# i.e. R[1] is returned for values &amp;lt; A[1], R[2] for &amp;lt; A[2], etc.
function searchsortedlasttree(A,R,var_name)  
    l = length(A)
    mid = iseven(l) ? l&amp;gt;&amp;gt;&amp;gt;1 : (l&amp;gt;&amp;gt;&amp;gt;1)+1
    # Handle base case
    if mid == 1
        if l == 1
            return :($var_name &amp;lt; $(A[1]) ? $(R[1]) : $(R[2]))
        else # l == 2
            return :($var_name &amp;lt; $(A[1]) ? $(R[1]) : 
                     $var_name &amp;lt; $(A[2]) ? $(R[2]) : $(R[3]))
        end
    end
    iftree = Expr(:if)
    iftree.args = Array(Any,3)
    iftree.args[1] = :($var_name &amp;lt; $(A[mid])) # condition
    iftree.args[2] = searchsortedlasttree(A[1:mid-1],R[1:mid],var_name)
    iftree.args[3] = searchsortedlasttree(A[mid+1:end],R[mid+1:end],var_name)
    return iftree
end

macro leapstree(a,r,var_name)  
    # var_name is the variable name that will be passed in thru the parent function
    A = eval(a)  # safe to eval here because 'a' is known at compile time
    R = eval(r)  # same here
    ret = Expr(:block)  # we'll store everything in a big block expression
    # First we manually handle before and after the ends of our leaps array
    push!(ret.args,:($var_name &amp;lt; $(A[1]) &amp;amp;&amp;amp; return 0))
    push!(ret.args,:($var_name &amp;gt;= $(A[end]) &amp;amp;&amp;amp; return $(endof(A)*1000)))
    # Now we call the recursive searchsortedlasttree to get the rest
    push!(ret.args,searchsortedlasttree(A[2:(endof(A)-1)],R,var_name))
    return ret
end

const SETLEAPS = [62214480000000,62230377600000,62261913600000, 62293449600000,62324985600000,62356608000000,62388144000000, 62419680000000,62451216000000,62498476800000,62530012800000, 62561548800000,62624707200000,62703676800000,62766835200000, 62798371200000,62845632000000,62877168000000,62908704000000, 62956137600000,63003398400000,63050832000000,63271756800000, 63366451200000,63476784000000]

function setleaps(ms)  
    # @leapstree as a macro ensures everything gets expanded
    # at compile time
    @leapstree(SETLEAPS,[1000:1000:((endof(SETLEAPS)-1)*1000)],ms)
end  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I tried to add useful comments so you can follow what's going on above, but basically we have our &lt;code&gt;setleaps&lt;/code&gt; function that will determine how many leap seconds need to be corrected given a &lt;code&gt;totalms&lt;/code&gt; value. &lt;code&gt;setleaps&lt;/code&gt; is built at compile time by expanding the macro &lt;code&gt;leapstree&lt;/code&gt; which manually builds a binary search tree given the &lt;code&gt;SETLEAPS&lt;/code&gt; array of leap second instants and the return values for each slot. The binary search tree is built primarily by &lt;code&gt;searchsortedlasttree&lt;/code&gt;, which is similar to the &lt;a href="https://github.com/JuliaLang/julia/blob/master/base/sort.jl#L144"&gt;searchsortedlast&lt;/a&gt; implementation, except instead of returning &lt;em&gt;values&lt;/em&gt;, it returns &lt;em&gt;expressions&lt;/em&gt;. It is also built recursively to handle any depth of tree.&lt;/p&gt;

&lt;p&gt;Hmmm....seems like a bit of hassle, does it really improve things much? Well, for one thing, the above code is &lt;em&gt;robust&lt;/em&gt;, I've written it once and don't have to do anything more except tack on the next leap second to &lt;code&gt;SETLEAPS&lt;/code&gt; the next time one is announced. Our manually built binary search tree will be rebalanced automatically. What about performance?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;julia&amp;gt; @time [searchsortedlast(SETLEAPS, 63366451200000) for i = 1:10000000];  
elapsed time: 0.50351136 seconds (80000048 bytes allocated)

julia&amp;gt; @time [setleaps(63366451200000) for i = 1:10000000];  
elapsed time: 0.067922885 seconds (80000048 bytes allocated)  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Boom! Almost 10x!&lt;/p&gt;

&lt;p&gt;Anyway, I thought this was an interesting application of meta-programming in Julia where we're basically taking static data and generating an operation on a data structure in the code itself. A &lt;a href="https://groups.google.com/forum/#!searchin/julia-users/long-term/julia-users/9MDGStifKy8/IqSzaG2wxk4J"&gt;recent thread&lt;/a&gt; in the Julia-Users forum asked about the long-term hopes of Julia achieving true C/Fortran performance (currently usually 1.2-2x). I think another strong point to the argument of Julia vs. C/Fortran is what I've shown above. Powerful language design choices like strong meta-programming functionality allows you to do things in Julia that would be almost impossible in many other languages, &lt;em&gt;but that also allow for unique performance gains&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Stay tuned for more adventures in Julia land and maybe for my next post, I'll talk a little more about Julia's code introspection tools and how they can be so darn handy while developing. As a teaser, here's the generated code for our &lt;code&gt;setleaps&lt;/code&gt; method above:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In  [62]: @code_lowered setleaps(62561548800000)

Out [62]: 1-element Array{Any,1}:  
 :($(Expr(:lambda, {:ms}, {{},{{:ms,:Any,0}},{}}, :(begin  # In[30], line 43:
        unless ms &amp;lt; 62214480000000 goto 0
        return 0
        0: 
        unless ms &amp;gt;= 63476784000000 goto 1
        return 25000
        1: 
        unless ms &amp;lt; 62624707200000 goto 15
        unless ms &amp;lt; 62388144000000 goto 8
        unless ms &amp;lt; 62293449600000 goto 4
        unless ms &amp;lt; 62230377600000 goto 2
        return 1000
        2: 
        unless ms &amp;lt; 62261913600000 goto 3
        return 2000
        3: 
        return 3000
        goto 7
        4: 
        unless ms &amp;lt; 62324985600000 goto 5
        return 4000
        5: 
        unless ms &amp;lt; 62356608000000 goto 6
        return 5000
        6: 
        return 6000
        7: 
        goto 14
        8: 
        unless ms &amp;lt; 62498476800000 goto 11
        unless ms &amp;lt; 62419680000000 goto 9
        return 7000
        9: 
        unless ms &amp;lt; 62451216000000 goto 10
        return 8000
        10: 
        return 9000
        goto 14
        11: 
        unless ms &amp;lt; 62530012800000 goto 12
        return 10000
        12: 
        unless ms &amp;lt; 62561548800000 goto 13
        return 11000
        13: 
        return 12000
        14: 
        goto 28
        15: 
        unless ms &amp;lt; 62908704000000 goto 22
        unless ms &amp;lt; 62798371200000 goto 18
        unless ms &amp;lt; 62703676800000 goto 16
        return 13000
        16: 
        unless ms &amp;lt; 62766835200000 goto 17
        return 14000
        17: 
        return 15000
        goto 21
        18: 
        unless ms &amp;lt; 62845632000000 goto 19
        return 16000
        19: 
        unless ms &amp;lt; 62877168000000 goto 20
        return 17000
        20: 
        return 18000
        21: 
        goto 28
        22: 
        unless ms &amp;lt; 63050832000000 goto 25
        unless ms &amp;lt; 62956137600000 goto 23
        return 19000
        23: 
        unless ms &amp;lt; 63003398400000 goto 24
        return 20000
        24: 
        return 21000
        goto 28
        25: 
        unless ms &amp;lt; 63271756800000 goto 26
        return 22000
        26: 
        unless ms &amp;lt; 63366451200000 goto 27
        return 23000
        27: 
        return 24000
        28: 
    end))))
&lt;/code&gt;&lt;/pre&gt;</content:encoded></item></channel></rss>